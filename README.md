# Vietnam Stock Big Data Pipeline

## Real-time Stock Data Pipeline v·ªõi Kafka, Spark v√† TimescaleDB

Pipeline chuy√™n nghi·ªáp ƒë·ªÉ thu th·∫≠p, x·ª≠ l√Ω v√† ph√¢n t√≠ch d·ªØ li·ªáu ch·ª©ng kho√°n Vi·ªát Nam real-time v·ªõi ki·∫øn tr√∫c Big Data hi·ªán ƒë·∫°i.

## Ki·∫øn Tr√∫c H·ªá Th·ªëng

![System Architecture](images/system-architecture.png)

```
VNStock API ‚Üí Kafka Producer ‚Üí Kafka Topics ‚Üí Spark Structured Streaming ‚Üí TimescaleDB ‚Üí Streamlit Dashboard
     ‚Üì              ‚Üì                ‚Üì                    ‚Üì                    ‚Üì              ‚Üì
D·ªØ li·ªáu th√¥ ‚Üí H√†ng ƒë·ª£i tin nh·∫Øn ‚Üí Streaming ‚Üí X·ª≠ l√Ω Big Data ‚Üí Time-series DB ‚Üí Giao di·ªán ph√¢n t√≠ch
                                                                         ‚Üì
                                                              Kafka UI / Spark UI
                                                              (Qu·∫£n l√Ω & Monitoring)
```

### Lu·ªìng D·ªØ Li·ªáu (Data Flow)

H·ªá th·ªëng ho·∫°t ƒë·ªông theo ki·∫øn tr√∫c pipeline real-time v·ªõi c√°c giai ƒëo·∫°n x·ª≠ l√Ω tu·∫ßn t·ª±:

#### 1. **Thu Th·∫≠p D·ªØ Li·ªáu (Data Ingestion)**
- **VNStock API** ‚Üí **Kafka Producer**: Producer thu th·∫≠p d·ªØ li·ªáu th√¥ (raw data) t·ª´ VNStock API v·ªõi t·∫ßn su·∫•t 10 gi√¢y/l·∫ßn cho 200+ m√£ c·ªï phi·∫øu.
- **Kafka Producer** ‚Üí **Kafka Topics**: Producer publish messages v√†o Kafka topic `realtime_quotes` d∆∞·ªõi d·∫°ng JSON, t·∫°o h√†ng ƒë·ª£i tin nh·∫Øn (message queue) ƒë·ªÉ x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô.

#### 2. **X·ª≠ L√Ω D·ªØ Li·ªáu (Data Processing)**
- **Kafka Topics** ‚Üí **Spark Structured Streaming**: Spark consumer ƒë·ªçc d·ªØ li·ªáu streaming t·ª´ Kafka v·ªõi micro-batching (trigger 15 gi√¢y), x·ª≠ l√Ω v√† transform d·ªØ li·ªáu.
- **Spark Structured Streaming** ‚Üí **TimescaleDB**: Spark th·ª±c hi·ªán x·ª≠ l√Ω Big Data (cleaning, validation, aggregation) v√† l∆∞u tr·ªØ v√†o TimescaleDB d∆∞·ªõi d·∫°ng time-series data v·ªõi batch size 10,000 records.

#### 3. **Tr·ª±c Quan H√≥a (Visualization)**
- **TimescaleDB** ‚Üí **Streamlit Dashboard**: Dashboard truy v·∫•n d·ªØ li·ªáu t·ª´ TimescaleDB v√† hi·ªÉn th·ªã giao di·ªán ph√¢n t√≠ch real-time v·ªõi c√°c bi·ªÉu ƒë·ªì candlestick, volume, moving averages.

#### 4. **Gi√°m S√°t v√† Qu·∫£n L√Ω (Monitoring & Management)**
- **Kafka UI / Spark UI**: Cung c·∫•p giao di·ªán web ƒë·ªÉ:
  - **Gi√°m s√°t Kafka**: Theo d√µi topics, partitions, consumer lag, message throughput.
  - **Qu·∫£n l√Ω & Monitoring Spark**: Monitor Spark jobs, stages, tasks, executor resources, streaming query statistics.
  - Truy c·∫≠p qua Nginx Reverse Proxy v·ªõi Basic Authentication.

### C√°c Th√†nh Ph·∫ßn Ch√≠nh

- **VNStock API**: Thu th·∫≠p d·ªØ li·ªáu real-time t·ª´ th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam.
- **Apache Kafka**: Message streaming platform cho real-time data ingestion.
- **Apache Spark**: Structured Streaming engine cho big data processing.
- **TimescaleDB**: Time-series database (PostgreSQL extension) t·ªëi ∆∞u cho analytics.
- **Streamlit Dashboard**: Real-time dashboard v·ªõi giao di·ªán SSI-style chuy√™n nghi·ªáp.
- **Kafka UI**: Web interface ƒë·ªÉ qu·∫£n l√Ω Kafka topics, partitions, consumer lag.
- **Spark UI**: Giao di·ªán monitor Spark jobs, stages, tasks v√† cluster resources.
- **Nginx Reverse Proxy**: Unified access point v·ªõi Basic Authentication cho c√°c UIs.

## T√≠nh NƒÉng Ch√≠nh

### Real-time Data Collection
- Thu th·∫≠p d·ªØ li·ªáu t·ª´ **200+ m√£ c·ªï phi·∫øu** real-time.
- Collection interval: **10 gi√¢y**.
- Parallel fetching v·ªõi **24 workers**.
- T·ª± ƒë·ªông l·ªçc v√† validate tickers.

### Big Data Processing
- **Spark Structured Streaming** v·ªõi micro-batching (15 gi√¢y).
- X·ª≠ l√Ω **50,000+ messages** m·ªói trigger.
- **200 shuffle partitions** cho high throughput.
- **10,000 batch size** cho JDBC writes.
- Checkpointing ƒë·ªÉ ƒë·∫£m b·∫£o fault tolerance.

### Time-series Database
- **TimescaleDB** hypertables cho d·ªØ li·ªáu time-series.
- T·ªëi ∆∞u cho queries theo th·ªùi gian.
- Compression v√† retention policies.
- Indexes t·ªëi ∆∞u cho performance.

### Professional Dashboard
- **SSI-style** giao di·ªán chuy√™n nghi·ªáp.
- **Real-time updates** v·ªõi auto-refresh.
- **Candlestick charts** v·ªõi MA5, MA20.
- **Volume charts** v·ªõi m√†u s·∫Øc professional.
- **Ph√¢n t√≠ch c·ªï phi·∫øu** chi ti·∫øt.
- **Top/Worst performers** tracking.

## C√†i ƒê·∫∑t v√† Tri·ªÉn Khai

### 1. Y√™u C·∫ßu H·ªá Th·ªëng

- **Docker** & **Docker Compose** (v2.0+).
- **RAM**: T·ªëi thi·ªÉu 8GB (Khuy·∫øn ngh·ªã 16GB+).
- **CPU**: T·ªëi thi·ªÉu 4 cores (Khuy·∫øn ngh·ªã 8+ cores).
- **Disk**: T·ªëi thi·ªÉu 50GB free space.
- **Network**: Internet connection ƒë·ªÉ truy c·∫≠p VNStock API.

### 2. Clone Repository

```bash
git clone <repository-url>
cd vietnam-stock-pipeline
```

### 3. C·∫•u H√¨nh

T·∫°o file `.env` trong th∆∞ m·ª•c g·ªëc v·ªõi c√°c bi·∫øn m√¥i tr∆∞·ªùng c·∫ßn thi·∫øt:

```bash
POSTGRES_PASSWORD=<your_password>
KAFKA_UI_PASSWORD=<your_password>
```

**L∆∞u √Ω**: File `.env` ƒë√£ ƒë∆∞·ª£c th√™m v√†o `.gitignore` v√† kh√¥ng ƒë∆∞·ª£c commit v√†o git.

### 4. Tri·ªÉn Khai Pipeline

```bash
# Kh·ªüi ƒë·ªông t·∫•t c·∫£ services
docker-compose -f docker-compose-timescaledb.yml up -d

# Xem tr·∫°ng th√°i
docker-compose -f docker-compose-timescaledb.yml ps

# Xem logs
docker-compose -f docker-compose-timescaledb.yml logs -f [service-name]
```

### 5. Truy C·∫≠p Services

| Service | URL | Credentials | M√¥ T·∫£ |
|---------|-----|-------------|-------|
| **Dashboard** | http://localhost:8501 | Kh√¥ng c·∫ßn | Streamlit dashboard |
| **UI Proxy** | http://localhost:8080 | admin / <password> | Unified access cho Spark UI & Kafka UI |
| **Spark UI** | http://localhost:8080/spark/ | admin / <password> | Monitor Spark jobs |
| **Kafka UI** | http://localhost:8080/kafka/ | admin / <password> | Qu·∫£n l√Ω Kafka topics |
| **TimescaleDB** | localhost:5433 | stock_app / <password> | Database |
| **Kafka** | localhost:9092 | - | Message broker |

**L∆∞u √Ω**: Spark UI v√† Kafka UI c≈©ng c√≥ th·ªÉ truy c·∫≠p tr·ª±c ti·∫øp qua:
- Spark UI: http://127.0.0.1:4041 (ch·ªâ localhost)
- Kafka UI: http://127.0.0.1:8081 (ch·ªâ localhost)

## Services v√† Ports

| Service | Container Name | Port (External) | Port (Internal) | Status |
|---------|---------------|-----------------|-----------------|--------|
| **TimescaleDB** | vietnam-stock-timescaledb | 5433 | 5432 | Healthy |
| **Zookeeper** | vietnam-stock-zookeeper | - | 2181 | Healthy |
| **Kafka** | vietnam-stock-kafka | 9092, 9093 | 29092 | Healthy |
| **Real Data Producer** | real-data-producer-vn30 | - | - | Running |
| **Spark Consumer** | vietnam-stock-spark-consumer | 4041 | 4040 | Healthy |
| **Dashboard** | vietnam-stock-dashboard | 8501 | 8501 | Healthy |
| **Kafka UI** | vietnam-stock-kafka-ui | 8081 | 8080 | Healthy |
| **UI Proxy** | vietnam-stock-ui-proxy | 8080 | 80 | Running |

## Database Schema

### B·∫£ng Ch√≠nh: `realtime_quotes`

```sql
CREATE TABLE realtime_quotes (
    time TIMESTAMPTZ NOT NULL,           -- Primary timestamp
    ticker VARCHAR(10) NOT NULL,          -- Stock symbol
    price DECIMAL(15,2),                  -- Current price
    volume BIGINT,                        -- Trading volume
    change DECIMAL(15,2),                 -- Price change
    percent_change DECIMAL(10,4),         -- Percentage change
    high DECIMAL(15,2),                   -- Day high
    low DECIMAL(15,2),                    -- Day low
    open_price DECIMAL(15,2),             -- Open price
    close_price DECIMAL(15,2),            -- Close price
    quote_time VARCHAR(50),               -- Quote timestamp from API
    ingest_time TIMESTAMPTZ,              -- Ingestion timestamp
    data_source VARCHAR(50) DEFAULT 'vnstock'
);

-- Convert to hypertable for time-series optimization
SELECT create_hypertable('realtime_quotes', 'time');
```

### Indexes

- Primary key: `(time, ticker)`.
- Indexes tr√™n `ticker`, `time` cho query performance.
- T·ªëi ∆∞u cho time-range queries.

## Kafka Configuration

### Topic: `realtime_quotes`

- **Partitions**: 12 (scaled for high throughput).
- **Replication Factor**: 1.
- **Retention**: 7 days.
- **Message Format**: JSON.

### Producer Configuration

- **Collection Interval**: 10 gi√¢y.
- **Max Workers**: 24 (parallel fetching).
- **Tickers**: 200+ m√£ c·ªï phi·∫øu (t·ª´ `data/top200_tickers.txt`).
- **Batch Size**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh.

### Consumer Configuration (Spark)

- **Trigger**: 15 seconds.
- **Max Offsets Per Trigger**: 50,000.
- **Shuffle Partitions**: 200.
- **JDBC Batch Size**: 10,000.
- **JDBC Write Partitions**: 8.

## Dashboard Features

### Trang Ch·ªß - T·ªïng Quan
- **T·ªïng Quan Th·ªã Tr∆∞·ªùng**: S·ªë li·ªáu t·ªïng h·ª£p, s·ªë l∆∞·ª£ng m√£, t·ªïng kh·ªëi l∆∞·ª£ng.
- **Ph√¢n B·ªë Hi·ªáu Su·∫•t**: Bi·ªÉu ƒë·ªì tƒÉng/gi·∫£m/kh√¥ng ƒë·ªïi.
- **Real-time Metrics**: C·∫≠p nh·∫≠t t·ª± ƒë·ªông.

### B·∫£ng Gi√° Real-time
- T√¨m ki·∫øm v√† l·ªçc m√£ c·ªï phi·∫øu.
- S·∫Øp x·∫øp theo kh·ªëi l∆∞·ª£ng, thay ƒë·ªïi %, gi√°.
- Hi·ªÉn th·ªã 50-500 m√£.
- Lo·∫°i b·ªè duplicate entries.

### Top Performers
- Top 20 c·ªï phi·∫øu tƒÉng gi√° m·∫°nh nh·∫•t.
- Bi·ªÉu ƒë·ªì v√† b·∫£ng chi ti·∫øt.
- Real-time updates.

### Ph√¢n T√≠ch C·ªï Phi·∫øu
- **Combined Chart**: Candlestick + Volume tr√™n c√πng m·ªôt chart.
- **Moving Averages**: MA5 (cyan) v√† MA20 (orange).
- **Volume Chart**: M√†u s·∫Øc professional (cyan #00BCD4).
- **Time Range**: 1 ng√†y, 7 ng√†y, 30 ng√†y.
- **Auto-refresh**: T√πy ch·ªçn t·ª± ƒë·ªông l√†m m·ªõi sau 10 gi√¢y.
- **Th√¥ng tin gi√° hi·ªán t·∫°i**: Gi√°, thay ƒë·ªïi, ph·∫ßn trƒÉm.

## Configuration

### Environment Variables

#### Real Data Producer
```bash
KAFKA_BOOTSTRAP_SERVERS=kafka:29092
COLLECTION_INTERVAL=10          # seconds
MAX_WORKERS=24                  # parallel workers
LOG_LEVEL=INFO
TICKERS_FILE=/app/data/top200_tickers.txt
```

#### Spark Consumer
```bash
KAFKA_BOOTSTRAP_SERVERS=kafka:29092
POSTGRES_URL=jdbc:postgresql://timescaledb:5432/stock_db
POSTGRES_USER=stock_app
POSTGRES_PASSWORD=<password>
SPARK_MASTER=local[*]
MAX_OFFSETS_PER_TRIGGER=50000
SPARK_SHUFFLE_PARTITIONS=200
JDBC_BATCH_SIZE=10000
JDBC_WRITE_PARTITIONS=8
STREAM_TRIGGER=15 seconds
```

#### Dashboard
```bash
POSTGRES_HOST=timescaledb
POSTGRES_PORT=5432
POSTGRES_DB=stock_db
POSTGRES_USER=stock_app
POSTGRES_PASSWORD=<password>
```

## Monitoring & Logging

### Log Locations

- `./logs/`: T·∫•t c·∫£ application logs
- `./logs/kafka_producer.log`: Producer logs
- `./logs/spark_consumer.log`: Spark consumer logs
- `./logs/dashboard.log`: Dashboard logs

### Health Checks

T·∫•t c·∫£ services c√≥ health checks:
- **TimescaleDB**: `pg_isready`.
- **Kafka**: Topic listing.
- **Spark Consumer**: Spark UI API check.
- **Dashboard**: Streamlit health endpoint.

### Monitoring Commands

```bash
# Xem logs real-time
docker-compose -f docker-compose-timescaledb.yml logs -f [service-name]

# Xem resource usage
docker stats

# Xem tr·∫°ng th√°i services
docker-compose -f docker-compose-timescaledb.yml ps

# Monitor Kafka topics
docker exec vietnam-stock-kafka kafka-topics --bootstrap-server localhost:9092 --list

# Monitor consumer lag
# Qua Kafka UI: http://localhost:8080/kafka/
```

### Key Metrics to Monitor

#### Kafka Metrics
- **Messages per second**: Throughput.
- **Consumer lag**: ƒê·ªô tr·ªÖ x·ª≠ l√Ω.
- **Partition distribution**: C√¢n b·∫±ng t·∫£i.
- **Topic size**: Disk usage.

#### Spark Metrics
- **Processing time**: Th·ªùi gian x·ª≠ l√Ω m·ªói batch.
- **Throughput**: Records/second.
- **Input rate**: T·ª´ Kafka.
- **Output rate**: ƒê·∫øn TimescaleDB.
- **Shuffle read/write**: I/O performance.

#### Database Metrics
- **Records count**: T·ªïng s·ªë records.
- **Unique tickers**: S·ªë m√£ c·ªï phi·∫øu.
- **Data freshness**: Th·ªùi gian c·∫≠p nh·∫≠t g·∫ßn nh·∫•t.
- **Query performance**: Response time.

## Development

### Project Structure

```
vietnam-stock-pipeline/
‚îú‚îÄ‚îÄ kafka_producers/              # Data ingestion
‚îÇ   ‚îú‚îÄ‚îÄ real_data_producer.py     # Real-time data producer
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ consumers/                    # Data processing
‚îÇ   ‚îî‚îÄ‚îÄ spark_structured_streaming_consumer.py
‚îú‚îÄ‚îÄ dashboard/                    # Streamlit dashboard
‚îÇ   ‚îî‚îÄ‚îÄ ssi_style_dashboard.py    # SSI-style professional dashboard
‚îú‚îÄ‚îÄ dockerfiles/                  # Docker configurations
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.kafka-producer
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.spark-consumer
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.dashboard
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile.nginx-reverse-proxy
‚îú‚îÄ‚îÄ scripts/                      # Management scripts
‚îÇ   ‚îî‚îÄ‚îÄ timescaledb_init.sql      # Database initialization
‚îú‚îÄ‚îÄ nginx/                        # Nginx configuration
‚îÇ   ‚îî‚îÄ‚îÄ conf.d/bigdata-ui.conf
‚îú‚îÄ‚îÄ data/                        # Data files
‚îÇ   ‚îî‚îÄ‚îÄ top200_tickers.txt        # List of 200 stock tickers
‚îú‚îÄ‚îÄ logs/                         # Application logs
‚îú‚îÄ‚îÄ docker-compose-timescaledb.yml # Main orchestration file
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ .env                          # Environment variables (not in git)
‚îî‚îÄ‚îÄ README.md                     # This file
```

### System Design
- Chi ti·∫øt ki·∫øn tr√∫c, workflows, l√Ω do ch·ªçn tech stack, tuning, b·∫£o m·∫≠t: xem `DOCS/SYSTEM_DESIGN.md`.

### Adding New Features

1. **Th√™m ticker m·ªõi**: C·∫≠p nh·∫≠t `data/top200_tickers.txt`.
2. **Thay ƒë·ªïi collection interval**: S·ª≠a `COLLECTION_INTERVAL` trong docker-compose.
3. **T·ªëi ∆∞u Spark**: ƒêi·ªÅu ch·ªânh `MAX_OFFSETS_PER_TRIGGER`, `SPARK_SHUFFLE_PARTITIONS`.
4. **Th√™m dashboard feature**: C·∫≠p nh·∫≠t `dashboard/ssi_style_dashboard.py`.

## Security

### Authentication

- **UI Proxy**: Basic Authentication (admin / <password>).
- **Kafka UI**: Login authentication.
- **Spark UI**: Qua proxy v·ªõi Basic Auth.
- **Dashboard**: Public (c√≥ th·ªÉ th√™m auth n·∫øu c·∫ßn).

### Network Security

- **Local-only ports**: Spark UI (4041) v√† Kafka UI (8081) ch·ªâ bind tr√™n 127.0.0.1 .
- **Public ports**: Dashboard (8501), TimescaleDB (5433), Kafka (9092).
- **Internal network**: Docker network ri√™ng cho services.

### Container Security

- Non-root users trong containers.
- Health checks enabled.
- Resource limits (n·∫øu c·∫ßn).
- Read-only volumes cho data files.

## Troubleshooting

### Common Issues

#### 1. Services kh√¥ng start
```bash
# Ki·ªÉm tra logs
docker-compose -f docker-compose-timescaledb.yml logs [service-name]

# Ki·ªÉm tra status
docker-compose -f docker-compose-timescaledb.yml ps

# Restart service
docker-compose -f docker-compose-timescaledb.yml restart [service-name]
```

#### 2. Database connection failed
```bash
# Ki·ªÉm tra TimescaleDB
docker exec vietnam-stock-timescaledb pg_isready -U stock_app -d stock_db

# Test connection
docker exec vietnam-stock-timescaledb psql -U stock_app -d stock_db -c "SELECT 1;"
```

#### 3. Kh√¥ng c√≥ d·ªØ li·ªáu trong dashboard
```bash
# Ki·ªÉm tra producer
docker logs real-data-producer-vn30 --tail 50

# Ki·ªÉm tra Kafka topics
docker exec vietnam-stock-kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic realtime_quotes --from-beginning --max-messages 10

# Ki·ªÉm tra database
docker exec vietnam-stock-timescaledb psql -U stock_app -d stock_db -c "SELECT COUNT(*) FROM realtime_quotes;"
```

#### 4. Spark consumer lag cao
- TƒÉng `MAX_OFFSETS_PER_TRIGGER`.
- TƒÉng `SPARK_SHUFFLE_PARTITIONS`.
- TƒÉng `JDBC_WRITE_PARTITIONS`.
- Ki·ªÉm tra resource usage.

### Performance Tuning

#### Kafka
- TƒÉng s·ªë partitions n·∫øu c·∫ßn.
- ƒêi·ªÅu ch·ªânh retention policy.
- Monitor consumer lag.

#### Spark
- TƒÉng shuffle partitions cho large datasets.
- ƒêi·ªÅu ch·ªânh batch size.
- T·ªëi ∆∞u checkpoint location..

#### TimescaleDB
- Compression policies.
- Retention policies.
- Index optimization.

## Support & Resources

### Documentation
- **README.md**: File n√†y
- **DOCS/SYSTEM_DESIGN.md**: Chi ti·∫øt ki·∫øn tr√∫c v√† thi·∫øt k·∫ø h·ªá th·ªëng

### Monitoring URLs
- Dashboard: http://localhost:8501
- UI Proxy: http://localhost:8080
- Spark UI: http://localhost:8080/spark/
- Kafka UI: http://localhost:8080/kafka/

### Useful Commands

```bash
# Xem t·∫•t c·∫£ logs
docker-compose -f docker-compose-timescaledb.yml logs -f

# Restart t·∫•t c·∫£ services
docker-compose -f docker-compose-timescaledb.yml restart

# Stop t·∫•t c·∫£ services
docker-compose -f docker-compose-timescaledb.yml stop

# Start t·∫•t c·∫£ services
docker-compose -f docker-compose-timescaledb.yml start

# Rebuild images
docker-compose -f docker-compose-timescaledb.yml build --no-cache

# Clean unused resources
docker system prune -f
```

## Roadmap

### Phase 1: Core Pipeline (Completed)
- [x] VNStock API integration.
- [x] Kafka streaming v·ªõi 200+ tickers.
- [x] Spark Structured Streaming.
- [x] TimescaleDB integration.
- [x] Professional Streamlit dashboard.
- [x] Kafka UI v√† Spark UI.
- [x] Security v·ªõi Basic Auth.

### Phase 2: Advanced Features (Planned)
- [ ] Machine learning predictions.
- [ ] Alert system (email/SMS).
- [ ] API endpoints (REST/GraphQL).
- [ ] Advanced analytics queries.
- [ ] Multi-exchange support.

### Phase 3: Enterprise Features (Future)
- [ ] High availability setup.
- [ ] Multi-region deployment.
- [ ] Real-time alerts.
- [ ] Custom indicators.
- [ ] Portfolio management.

## ü§ù Contributing

1. Fork the repository.
2. Create feature branch (`git checkout -b feature/AmazingFeature`).
3. Commit changes (`git commit -m 'Add some AmazingFeature'`).
4. Push to branch (`git push origin feature/AmazingFeature`).
5. Open Pull Request.

---

**Vietnam Stock Big Data Pipeline - Powered by Kafka + Spark + TimescaleDB + Streamlit**
