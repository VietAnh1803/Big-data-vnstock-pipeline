# ğŸ“ˆ Vietnam Stock Big Data Pipeline

## ğŸš€ Professional Real-time Stock Data Pipeline vá»›i Kafka, Spark vÃ  TimescaleDB

Pipeline chuyÃªn nghiá»‡p Ä‘á»ƒ thu tháº­p, xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u chá»©ng khoÃ¡n Viá»‡t Nam real-time vá»›i kiáº¿n trÃºc Big Data hiá»‡n Ä‘áº¡i, Ä‘Æ°á»£c thiáº¿t káº¿ bá»Ÿi Senior Data Engineer.

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

```
VNStock API â†’ Kafka Producer â†’ Kafka Topics â†’ Spark Structured Streaming â†’ TimescaleDB â†’ Streamlit Dashboard
     â†“              â†“                â†“                    â†“                    â†“              â†“
Dá»¯ liá»‡u thÃ´ â†’ HÃ ng Ä‘á»£i tin nháº¯n â†’ Streaming â†’ Xá»­ lÃ½ Big Data â†’ Time-series DB â†’ Giao diá»‡n phÃ¢n tÃ­ch
                                                                         â†“
                                                              Kafka UI / Spark UI
                                                              (Quáº£n lÃ½ & Monitoring)
```

### ğŸ“Š Luá»“ng Dá»¯ Liá»‡u (Data Flow)

Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng theo kiáº¿n trÃºc pipeline real-time vá»›i cÃ¡c giai Ä‘oáº¡n xá»­ lÃ½ tuáº§n tá»±:

#### 1. **Thu Tháº­p Dá»¯ Liá»‡u (Data Ingestion)**
- **VNStock API** â†’ **Kafka Producer**: Producer thu tháº­p dá»¯ liá»‡u thÃ´ (raw data) tá»« VNStock API vá»›i táº§n suáº¥t 10 giÃ¢y/láº§n cho 200+ mÃ£ cá»• phiáº¿u
- **Kafka Producer** â†’ **Kafka Topics**: Producer publish messages vÃ o Kafka topic `realtime_quotes` dÆ°á»›i dáº¡ng JSON, táº¡o hÃ ng Ä‘á»£i tin nháº¯n (message queue) Ä‘á»ƒ xá»­ lÃ½ báº¥t Ä‘á»“ng bá»™

#### 2. **Xá»­ LÃ½ Dá»¯ Liá»‡u (Data Processing)**
- **Kafka Topics** â†’ **Spark Structured Streaming**: Spark consumer Ä‘á»c dá»¯ liá»‡u streaming tá»« Kafka vá»›i micro-batching (trigger 15 giÃ¢y), xá»­ lÃ½ vÃ  transform dá»¯ liá»‡u
- **Spark Structured Streaming** â†’ **TimescaleDB**: Spark thá»±c hiá»‡n xá»­ lÃ½ Big Data (cleaning, validation, aggregation) vÃ  lÆ°u trá»¯ vÃ o TimescaleDB dÆ°á»›i dáº¡ng time-series data vá»›i batch size 10,000 records

#### 3. **Trá»±c Quan HÃ³a (Visualization)**
- **TimescaleDB** â†’ **Streamlit Dashboard**: Dashboard truy váº¥n dá»¯ liá»‡u tá»« TimescaleDB vÃ  hiá»ƒn thá»‹ giao diá»‡n phÃ¢n tÃ­ch real-time vá»›i cÃ¡c biá»ƒu Ä‘á»“ candlestick, volume, moving averages

#### 4. **GiÃ¡m SÃ¡t vÃ  Quáº£n LÃ½ (Monitoring & Management)**
- **Kafka UI / Spark UI**: Cung cáº¥p giao diá»‡n web Ä‘á»ƒ:
  - **GiÃ¡m sÃ¡t Kafka**: Theo dÃµi topics, partitions, consumer lag, message throughput
  - **Quáº£n lÃ½ & Monitoring Spark**: Monitor Spark jobs, stages, tasks, executor resources, streaming query statistics
  - Truy cáº­p qua Nginx Reverse Proxy vá»›i Basic Authentication

### ğŸ”§ CÃ¡c ThÃ nh Pháº§n ChÃ­nh

- **ğŸ“Š VNStock API**: Thu tháº­p dá»¯ liá»‡u real-time tá»« thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam
- **âš¡ Apache Kafka**: Message streaming platform cho real-time data ingestion
- **ğŸ”¥ Apache Spark**: Structured Streaming engine cho big data processing
- **ğŸ—„ï¸ TimescaleDB**: Time-series database (PostgreSQL extension) tá»‘i Æ°u cho analytics
- **ğŸ“ˆ Streamlit Dashboard**: Real-time dashboard vá»›i giao diá»‡n SSI-style chuyÃªn nghiá»‡p
- **ğŸŒ Kafka UI**: Web interface Ä‘á»ƒ quáº£n lÃ½ Kafka topics, partitions, consumer lag
- **ğŸ” Spark UI**: Giao diá»‡n monitor Spark jobs, stages, tasks vÃ  cluster resources
- **ğŸ”’ Nginx Reverse Proxy**: Unified access point vá»›i Basic Authentication cho cÃ¡c UIs

## ğŸ“‹ TÃ­nh NÄƒng ChÃ­nh

### âœ… Real-time Data Collection
- Thu tháº­p dá»¯ liá»‡u tá»« **200+ mÃ£ cá»• phiáº¿u** real-time
- Collection interval: **10 giÃ¢y**
- Parallel fetching vá»›i **24 workers**
- Tá»± Ä‘á»™ng lá»c vÃ  validate tickers

### âœ… Big Data Processing
- **Spark Structured Streaming** vá»›i micro-batching (15 giÃ¢y)
- Xá»­ lÃ½ **50,000+ messages** má»—i trigger
- **200 shuffle partitions** cho high throughput
- **10,000 batch size** cho JDBC writes
- Checkpointing Ä‘á»ƒ Ä‘áº£m báº£o fault tolerance

### âœ… Time-series Database
- **TimescaleDB** hypertables cho dá»¯ liá»‡u time-series
- Tá»‘i Æ°u cho queries theo thá»i gian
- Compression vÃ  retention policies
- Indexes tá»‘i Æ°u cho performance

### âœ… Professional Dashboard
- **SSI-style** giao diá»‡n chuyÃªn nghiá»‡p
- **Real-time updates** vá»›i auto-refresh
- **Candlestick charts** vá»›i MA5, MA20
- **Volume charts** vá»›i mÃ u sáº¯c professional
- **PhÃ¢n tÃ­ch cá»• phiáº¿u** chi tiáº¿t
- **Top/Worst performers** tracking

## ğŸš€ CÃ i Äáº·t vÃ  Triá»ƒn Khai

### 1. YÃªu Cáº§u Há»‡ Thá»‘ng

- **Docker** & **Docker Compose** (v2.0+)
- **RAM**: Tá»‘i thiá»ƒu 8GB (Khuyáº¿n nghá»‹ 16GB+)
- **CPU**: Tá»‘i thiá»ƒu 4 cores (Khuyáº¿n nghá»‹ 8+ cores)
- **Disk**: Tá»‘i thiá»ƒu 50GB free space
- **Network**: Internet connection Ä‘á»ƒ truy cáº­p VNStock API

### 2. Clone Repository

```bash
git clone <repository-url>
cd vietnam-stock-pipeline
```

### 3. Cáº¥u HÃ¬nh

Táº¡o file `.env` trong thÆ° má»¥c gá»‘c vá»›i cÃ¡c biáº¿n mÃ´i trÆ°á»ng cáº§n thiáº¿t:

```bash
POSTGRES_PASSWORD=<your_password>
KAFKA_UI_PASSWORD=<your_password>
```

**LÆ°u Ã½**: File `.env` Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o `.gitignore` vÃ  khÃ´ng Ä‘Æ°á»£c commit vÃ o git.

### 4. Triá»ƒn Khai Pipeline

```bash
# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker-compose -f docker-compose-timescaledb.yml up -d

# Xem tráº¡ng thÃ¡i
docker-compose -f docker-compose-timescaledb.yml ps

# Xem logs
docker-compose -f docker-compose-timescaledb.yml logs -f [service-name]
```

### 5. Truy Cáº­p Services

| Service | URL | Credentials | MÃ´ Táº£ |
|---------|-----|-------------|-------|
| **Dashboard** | http://localhost:8501 | KhÃ´ng cáº§n | Streamlit dashboard |
| **UI Proxy** | http://localhost:8080 | admin / <password> | Unified access cho Spark UI & Kafka UI |
| **Spark UI** | http://localhost:8080/spark/ | admin / <password> | Monitor Spark jobs |
| **Kafka UI** | http://localhost:8080/kafka/ | admin / <password> | Quáº£n lÃ½ Kafka topics |
| **TimescaleDB** | localhost:5433 | stock_app / <password> | Database |
| **Kafka** | localhost:9092 | - | Message broker |

**LÆ°u Ã½**: Spark UI vÃ  Kafka UI cÅ©ng cÃ³ thá»ƒ truy cáº­p trá»±c tiáº¿p qua:
- Spark UI: http://127.0.0.1:4041 (chá»‰ localhost)
- Kafka UI: http://127.0.0.1:8081 (chá»‰ localhost)

## ğŸ“Š Services vÃ  Ports

| Service | Container Name | Port (External) | Port (Internal) | Status |
|---------|---------------|-----------------|-----------------|--------|
| **TimescaleDB** | vietnam-stock-timescaledb | 5433 | 5432 | âœ… Healthy |
| **Zookeeper** | vietnam-stock-zookeeper | - | 2181 | âœ… Healthy |
| **Kafka** | vietnam-stock-kafka | 9092, 9093 | 29092 | âœ… Healthy |
| **Real Data Producer** | real-data-producer-vn30 | - | - | âœ… Running |
| **Spark Consumer** | vietnam-stock-spark-consumer | 4041 | 4040 | âœ… Healthy |
| **Dashboard** | vietnam-stock-dashboard | 8501 | 8501 | âœ… Healthy |
| **Kafka UI** | vietnam-stock-kafka-ui | 8081 | 8080 | âœ… Healthy |
| **UI Proxy** | vietnam-stock-ui-proxy | 8080 | 80 | âœ… Running |

## ğŸ—„ï¸ Database Schema

### Báº£ng ChÃ­nh: `realtime_quotes`

```sql
CREATE TABLE realtime_quotes (
    time TIMESTAMPTZ NOT NULL,           -- Primary timestamp
    ticker VARCHAR(10) NOT NULL,          -- Stock symbol
    price DECIMAL(15,2),                  -- Current price
    volume BIGINT,                        -- Trading volume
    change DECIMAL(15,2),                 -- Price change
    percent_change DECIMAL(10,4),         -- Percentage change
    high DECIMAL(15,2),                   -- Day high
    low DECIMAL(15,2),                    -- Day low
    open_price DECIMAL(15,2),             -- Open price
    close_price DECIMAL(15,2),            -- Close price
    quote_time VARCHAR(50),               -- Quote timestamp from API
    ingest_time TIMESTAMPTZ,              -- Ingestion timestamp
    data_source VARCHAR(50) DEFAULT 'vnstock'
);

-- Convert to hypertable for time-series optimization
SELECT create_hypertable('realtime_quotes', 'time');
```

### Indexes

- Primary key: `(time, ticker)`
- Indexes trÃªn `ticker`, `time` cho query performance
- Tá»‘i Æ°u cho time-range queries

## âš¡ Kafka Configuration

### Topic: `realtime_quotes`

- **Partitions**: 12 (scaled for high throughput)
- **Replication Factor**: 1
- **Retention**: 7 days
- **Message Format**: JSON

### Producer Configuration

- **Collection Interval**: 10 giÃ¢y
- **Max Workers**: 24 (parallel fetching)
- **Tickers**: 200+ mÃ£ cá»• phiáº¿u (tá»« `data/top200_tickers.txt`)
- **Batch Size**: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh

### Consumer Configuration (Spark)

- **Trigger**: 15 seconds
- **Max Offsets Per Trigger**: 50,000
- **Shuffle Partitions**: 200
- **JDBC Batch Size**: 10,000
- **JDBC Write Partitions**: 8

## ğŸ“ˆ Dashboard Features

### ğŸ  Trang Chá»§ - Tá»•ng Quan
- **Tá»•ng Quan Thá»‹ TrÆ°á»ng**: Sá»‘ liá»‡u tá»•ng há»£p, sá»‘ lÆ°á»£ng mÃ£, tá»•ng khá»‘i lÆ°á»£ng
- **PhÃ¢n Bá»‘ Hiá»‡u Suáº¥t**: Biá»ƒu Ä‘á»“ tÄƒng/giáº£m/khÃ´ng Ä‘á»•i
- **Real-time Metrics**: Cáº­p nháº­t tá»± Ä‘á»™ng

### ğŸ“‹ Báº£ng GiÃ¡ Real-time
- TÃ¬m kiáº¿m vÃ  lá»c mÃ£ cá»• phiáº¿u
- Sáº¯p xáº¿p theo khá»‘i lÆ°á»£ng, thay Ä‘á»•i %, giÃ¡
- Hiá»ƒn thá»‹ 50-500 mÃ£
- Loáº¡i bá» duplicate entries

### ğŸš€ Top Performers
- Top 20 cá»• phiáº¿u tÄƒng giÃ¡ máº¡nh nháº¥t
- Biá»ƒu Ä‘á»“ vÃ  báº£ng chi tiáº¿t
- Real-time updates

### ğŸ” PhÃ¢n TÃ­ch Cá»• Phiáº¿u
- **Combined Chart**: Candlestick + Volume trÃªn cÃ¹ng má»™t chart
- **Moving Averages**: MA5 (cyan) vÃ  MA20 (orange)
- **Volume Chart**: MÃ u sáº¯c professional (cyan #00BCD4)
- **Time Range**: 1 ngÃ y, 7 ngÃ y, 30 ngÃ y
- **Auto-refresh**: TÃ¹y chá»n tá»± Ä‘á»™ng lÃ m má»›i sau 10 giÃ¢y
- **ThÃ´ng tin giÃ¡ hiá»‡n táº¡i**: GiÃ¡, thay Ä‘á»•i, pháº§n trÄƒm

## ğŸ”§ Configuration

### Environment Variables

#### Real Data Producer
```bash
KAFKA_BOOTSTRAP_SERVERS=kafka:29092
COLLECTION_INTERVAL=10          # seconds
MAX_WORKERS=24                  # parallel workers
LOG_LEVEL=INFO
TICKERS_FILE=/app/data/top200_tickers.txt
```

#### Spark Consumer
```bash
KAFKA_BOOTSTRAP_SERVERS=kafka:29092
POSTGRES_URL=jdbc:postgresql://timescaledb:5432/stock_db
POSTGRES_USER=stock_app
POSTGRES_PASSWORD=<password>
SPARK_MASTER=local[*]
MAX_OFFSETS_PER_TRIGGER=50000
SPARK_SHUFFLE_PARTITIONS=200
JDBC_BATCH_SIZE=10000
JDBC_WRITE_PARTITIONS=8
STREAM_TRIGGER=15 seconds
```

#### Dashboard
```bash
POSTGRES_HOST=timescaledb
POSTGRES_PORT=5432
POSTGRES_DB=stock_db
POSTGRES_USER=stock_app
POSTGRES_PASSWORD=<password>
```

## ğŸ“Š Monitoring & Logging

### Log Locations

- `./logs/`: Táº¥t cáº£ application logs
- `./logs/kafka_producer.log`: Producer logs
- `./logs/spark_consumer.log`: Spark consumer logs
- `./logs/dashboard.log`: Dashboard logs

### Health Checks

Táº¥t cáº£ services cÃ³ health checks:
- **TimescaleDB**: `pg_isready`
- **Kafka**: Topic listing
- **Spark Consumer**: Spark UI API check
- **Dashboard**: Streamlit health endpoint

### Monitoring Commands

```bash
# Xem logs real-time
docker-compose -f docker-compose-timescaledb.yml logs -f [service-name]

# Xem resource usage
docker stats

# Xem tráº¡ng thÃ¡i services
docker-compose -f docker-compose-timescaledb.yml ps

# Monitor Kafka topics
docker exec vietnam-stock-kafka kafka-topics --bootstrap-server localhost:9092 --list

# Monitor consumer lag
# Qua Kafka UI: http://localhost:8080/kafka/
```

### Key Metrics to Monitor

#### Kafka Metrics
- **Messages per second**: Throughput
- **Consumer lag**: Äá»™ trá»… xá»­ lÃ½
- **Partition distribution**: CÃ¢n báº±ng táº£i
- **Topic size**: Disk usage

#### Spark Metrics
- **Processing time**: Thá»i gian xá»­ lÃ½ má»—i batch
- **Throughput**: Records/second
- **Input rate**: Tá»« Kafka
- **Output rate**: Äáº¿n TimescaleDB
- **Shuffle read/write**: I/O performance

#### Database Metrics
- **Records count**: Tá»•ng sá»‘ records
- **Unique tickers**: Sá»‘ mÃ£ cá»• phiáº¿u
- **Data freshness**: Thá»i gian cáº­p nháº­t gáº§n nháº¥t
- **Query performance**: Response time

## ğŸ› ï¸ Development

### Project Structure

```
vietnam-stock-pipeline/
â”œâ”€â”€ kafka_producers/              # Data ingestion
â”‚   â”œâ”€â”€ real_data_producer.py     # Real-time data producer
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ consumers/                    # Data processing
â”‚   â””â”€â”€ spark_structured_streaming_consumer.py
â”œâ”€â”€ dashboard/                    # Streamlit dashboard
â”‚   â””â”€â”€ ssi_style_dashboard.py    # SSI-style professional dashboard
â”œâ”€â”€ dockerfiles/                  # Docker configurations
â”‚   â”œâ”€â”€ Dockerfile.kafka-producer
â”‚   â”œâ”€â”€ Dockerfile.spark-consumer
â”‚   â”œâ”€â”€ Dockerfile.dashboard
â”‚   â””â”€â”€ Dockerfile.nginx-reverse-proxy
â”œâ”€â”€ scripts/                      # Management scripts
â”‚   â””â”€â”€ timescaledb_init.sql      # Database initialization
â”œâ”€â”€ nginx/                        # Nginx configuration
â”‚   â””â”€â”€ conf.d/bigdata-ui.conf
â”œâ”€â”€ data/                        # Data files
â”‚   â””â”€â”€ top200_tickers.txt        # List of 200 stock tickers
â”œâ”€â”€ logs/                         # Application logs
â”œâ”€â”€ docker-compose-timescaledb.yml # Main orchestration file
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env                          # Environment variables (not in git)
â””â”€â”€ README.md                     # This file
```

### System Design
- Chi tiáº¿t kiáº¿n trÃºc, workflows, lÃ½ do chá»n tech stack, tuning, báº£o máº­t: xem `DOCS/SYSTEM_DESIGN.md`.

### Adding New Features

1. **ThÃªm ticker má»›i**: Cáº­p nháº­t `data/top200_tickers.txt`
2. **Thay Ä‘á»•i collection interval**: Sá»­a `COLLECTION_INTERVAL` trong docker-compose
3. **Tá»‘i Æ°u Spark**: Äiá»u chá»‰nh `MAX_OFFSETS_PER_TRIGGER`, `SPARK_SHUFFLE_PARTITIONS`
4. **ThÃªm dashboard feature**: Cáº­p nháº­t `dashboard/ssi_style_dashboard.py`

## ğŸ”’ Security

### Authentication

- **UI Proxy**: Basic Authentication (admin / <password>)
- **Kafka UI**: Login authentication
- **Spark UI**: Qua proxy vá»›i Basic Auth
- **Dashboard**: Public (cÃ³ thá»ƒ thÃªm auth náº¿u cáº§n)

### Network Security

- **Local-only ports**: Spark UI (4041) vÃ  Kafka UI (8081) chá»‰ bind trÃªn 127.0.0.1
- **Public ports**: Dashboard (8501), TimescaleDB (5433), Kafka (9092)
- **Internal network**: Docker network riÃªng cho services

### Container Security

- Non-root users trong containers
- Health checks enabled
- Resource limits (náº¿u cáº§n)
- Read-only volumes cho data files

## ğŸš¨ Troubleshooting

### Common Issues

#### 1. Services khÃ´ng start
```bash
# Kiá»ƒm tra logs
docker-compose -f docker-compose-timescaledb.yml logs [service-name]

# Kiá»ƒm tra status
docker-compose -f docker-compose-timescaledb.yml ps

# Restart service
docker-compose -f docker-compose-timescaledb.yml restart [service-name]
```

#### 2. Database connection failed
```bash
# Kiá»ƒm tra TimescaleDB
docker exec vietnam-stock-timescaledb pg_isready -U stock_app -d stock_db

# Test connection
docker exec vietnam-stock-timescaledb psql -U stock_app -d stock_db -c "SELECT 1;"
```

#### 3. KhÃ´ng cÃ³ dá»¯ liá»‡u trong dashboard
```bash
# Kiá»ƒm tra producer
docker logs real-data-producer-vn30 --tail 50

# Kiá»ƒm tra Kafka topics
docker exec vietnam-stock-kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic realtime_quotes --from-beginning --max-messages 10

# Kiá»ƒm tra database
docker exec vietnam-stock-timescaledb psql -U stock_app -d stock_db -c "SELECT COUNT(*) FROM realtime_quotes;"
```

#### 4. Spark consumer lag cao
- TÄƒng `MAX_OFFSETS_PER_TRIGGER`
- TÄƒng `SPARK_SHUFFLE_PARTITIONS`
- TÄƒng `JDBC_WRITE_PARTITIONS`
- Kiá»ƒm tra resource usage

### Performance Tuning

#### Kafka
- TÄƒng sá»‘ partitions náº¿u cáº§n
- Äiá»u chá»‰nh retention policy
- Monitor consumer lag

#### Spark
- TÄƒng shuffle partitions cho large datasets
- Äiá»u chá»‰nh batch size
- Tá»‘i Æ°u checkpoint location

#### TimescaleDB
- Compression policies
- Retention policies
- Index optimization

## ğŸ“ Support & Resources

### Documentation
- **README.md**: File nÃ y
- **DOCS/SYSTEM_DESIGN.md**: Chi tiáº¿t kiáº¿n trÃºc vÃ  thiáº¿t káº¿ há»‡ thá»‘ng

### Monitoring URLs
- Dashboard: http://localhost:8501
- UI Proxy: http://localhost:8080
- Spark UI: http://localhost:8080/spark/
- Kafka UI: http://localhost:8080/kafka/

### Useful Commands

```bash
# Xem táº¥t cáº£ logs
docker-compose -f docker-compose-timescaledb.yml logs -f

# Restart táº¥t cáº£ services
docker-compose -f docker-compose-timescaledb.yml restart

# Stop táº¥t cáº£ services
docker-compose -f docker-compose-timescaledb.yml stop

# Start táº¥t cáº£ services
docker-compose -f docker-compose-timescaledb.yml start

# Rebuild images
docker-compose -f docker-compose-timescaledb.yml build --no-cache

# Clean unused resources
docker system prune -f
```

## ğŸ¯ Roadmap

### âœ… Phase 1: Core Pipeline (Completed)
- [x] VNStock API integration
- [x] Kafka streaming vá»›i 200+ tickers
- [x] Spark Structured Streaming
- [x] TimescaleDB integration
- [x] Professional Streamlit dashboard
- [x] Kafka UI vÃ  Spark UI
- [x] Security vá»›i Basic Auth

### ğŸ”„ Phase 2: Advanced Features (Planned)
- [ ] Machine learning predictions
- [ ] Alert system (email/SMS)
- [ ] API endpoints (REST/GraphQL)
- [ ] Advanced analytics queries
- [ ] Multi-exchange support

### ğŸš€ Phase 3: Enterprise Features (Future)
- [ ] High availability setup
- [ ] Multi-region deployment
- [ ] Real-time alerts
- [ ] Custom indicators
- [ ] Portfolio management

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

---

**ğŸš€ Vietnam Stock Big Data Pipeline - Powered by Kafka + Spark + TimescaleDB + Streamlit**

*Senior Data Engineer Level - Professional Real-time Stock Data Processing*
