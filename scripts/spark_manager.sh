#!/bin/bash

# Spark Cluster Manager for Vietnam Stock Pipeline
# H∆∞·ªõng d·∫´n qu·∫£n l√Ω Spark cluster t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao

echo "üöÄ SPARK CLUSTER MANAGER"
echo "========================"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to show help
show_help() {
    echo -e "${BLUE}üìã SPARK CLUSTER COMMANDS:${NC}"
    echo ""
    echo -e "${YELLOW}üîç MONITORING:${NC}"
    echo "  status     - Hi·ªÉn th·ªã tr·∫°ng th√°i cluster"
    echo "  ui         - M·ªü Spark Web UI"
    echo "  logs       - Xem logs c·ªßa Spark processor"
    echo "  metrics    - Hi·ªÉn th·ªã metrics chi ti·∫øt"
    echo ""
    echo -e "${YELLOW}‚öôÔ∏è  MANAGEMENT:${NC}"
    echo "  start      - Kh·ªüi ƒë·ªông Spark cluster"
    echo "  stop       - D·ª´ng Spark cluster"
    echo "  restart    - Kh·ªüi ƒë·ªông l·∫°i Spark cluster"
    echo "  scale      - Thay ƒë·ªïi s·ªë l∆∞·ª£ng workers"
    echo ""
    echo -e "${YELLOW}üìä JOBS:${NC}"
    echo "  submit     - Submit Spark job"
    echo "  kill       - D·ª´ng Spark job"
    echo "  list       - Li·ªát k√™ c√°c jobs ƒëang ch·∫°y"
    echo ""
    echo -e "${YELLOW}üîß CONFIGURATION:${NC}"
    echo "  config     - Hi·ªÉn th·ªã c·∫•u h√¨nh"
    echo "  optimize   - T·ªëi ∆∞u h√≥a cluster"
    echo "  test       - Test cluster connectivity"
    echo ""
    echo -e "${YELLOW}üìö LEARNING:${NC}"
    echo "  tutorial   - H∆∞·ªõng d·∫´n Spark c∆° b·∫£n"
    echo "  examples   - V√≠ d·ª• Spark jobs"
    echo "  docs       - T√†i li·ªáu Spark"
    echo ""
    echo -e "${GREEN}Usage: $0 [command]${NC}"
}

# Function to show cluster status
show_status() {
    echo -e "${BLUE}üìä SPARK CLUSTER STATUS${NC}"
    echo "=========================="
    
    echo -e "\n${YELLOW}üê≥ Docker Containers:${NC}"
    docker ps | grep spark
    
    echo -e "\n${YELLOW}üåê Web UIs:${NC}"
    echo "  Spark Master UI: http://localhost:8080"
    echo "  Spark Worker UI: http://localhost:8081"
    
    echo -e "\n${YELLOW}üìà Cluster Metrics:${NC}"
    curl -s http://localhost:8080 | grep -E "(Workers|Applications|Memory|Cores)" | head -5
    
    echo -e "\n${YELLOW}üíæ Worker Resources:${NC}"
    curl -s http://localhost:8081 | grep -E "(Memory|Cores|Running|Completed)" | head -5
}

# Function to open Spark UI
open_ui() {
    echo -e "${BLUE}üåê SPARK WEB UIs${NC}"
    echo "=================="
    echo ""
    echo -e "${GREEN}üìä Spark Master UI:${NC}"
    echo "  URL: http://localhost:8080"
    echo "  Features: Cluster overview, applications, workers"
    echo ""
    echo -e "${GREEN}‚öôÔ∏è  Spark Worker UI:${NC}"
    echo "  URL: http://localhost:8081"
    echo "  Features: Worker details, executors, logs"
    echo ""
    echo -e "${YELLOW}üí° Tips:${NC}"
    echo "  - Click v√†o 'Workers' ƒë·ªÉ xem chi ti·∫øt workers"
    echo "  - Click v√†o 'Applications' ƒë·ªÉ xem jobs ƒëang ch·∫°y"
    echo "  - Click v√†o 'Executors' ƒë·ªÉ xem resource usage"
    echo ""
    echo "M·ªü browser v√† truy c·∫≠p c√°c URL tr√™n ƒë·ªÉ qu·∫£n l√Ω Spark cluster!"
}

# Function to show logs
show_logs() {
    echo -e "${BLUE}üìã SPARK PROCESSOR LOGS${NC}"
    echo "=========================="
    echo ""
    echo -e "${YELLOW}Recent logs:${NC}"
    docker logs spark-processor --tail 20
    echo ""
    echo -e "${YELLOW}Follow logs (Ctrl+C to stop):${NC}"
    echo "docker logs spark-processor -f"
}

# Function to show metrics
show_metrics() {
    echo -e "${BLUE}üìä SPARK CLUSTER METRICS${NC}"
    echo "============================"
    
    echo -e "\n${YELLOW}üèóÔ∏è  Cluster Overview:${NC}"
    curl -s http://localhost:8080 | grep -A 5 -B 5 "Alive Workers"
    
    echo -e "\n${YELLOW}üíª Worker Details:${NC}"
    curl -s http://localhost:8081 | grep -A 3 -B 3 "Cores"
    
    echo -e "\n${YELLOW}üìà Applications:${NC}"
    curl -s http://localhost:8080 | grep -A 10 "Running Applications"
}

# Function to start cluster
start_cluster() {
    echo -e "${BLUE}üöÄ STARTING SPARK CLUSTER${NC}"
    echo "=========================="
    
    echo "Starting Spark Master..."
    docker compose up -d spark-master
    
    echo "Starting Spark Worker..."
    docker compose up -d spark-worker
    
    echo "Starting Spark Processor..."
    docker compose up -d spark-processor
    
    echo -e "\n${GREEN}‚úÖ Spark cluster started!${NC}"
    echo "Wait 30 seconds for full startup..."
    sleep 30
    show_status
}

# Function to stop cluster
stop_cluster() {
    echo -e "${BLUE}üõë STOPPING SPARK CLUSTER${NC}"
    echo "=========================="
    
    echo "Stopping Spark Processor..."
    docker compose stop spark-processor
    
    echo "Stopping Spark Worker..."
    docker compose stop spark-worker
    
    echo "Stopping Spark Master..."
    docker compose stop spark-master
    
    echo -e "\n${GREEN}‚úÖ Spark cluster stopped!${NC}"
}

# Function to restart cluster
restart_cluster() {
    echo -e "${BLUE}üîÑ RESTARTING SPARK CLUSTER${NC}"
    echo "============================"
    
    stop_cluster
    sleep 5
    start_cluster
}

# Function to test cluster
test_cluster() {
    echo -e "${BLUE}üß™ TESTING SPARK CLUSTER${NC}"
    echo "========================="
    
    echo -e "\n${YELLOW}1. Testing Master UI:${NC}"
    if curl -s http://localhost:8080 > /dev/null; then
        echo -e "${GREEN}‚úÖ Master UI accessible${NC}"
    else
        echo -e "${RED}‚ùå Master UI not accessible${NC}"
    fi
    
    echo -e "\n${YELLOW}2. Testing Worker UI:${NC}"
    if curl -s http://localhost:8081 > /dev/null; then
        echo -e "${GREEN}‚úÖ Worker UI accessible${NC}"
    else
        echo -e "${RED}‚ùå Worker UI not accessible${NC}"
    fi
    
    echo -e "\n${YELLOW}3. Testing Kafka Connection:${NC}"
    if docker exec spark-processor python3 -c "from kafka import KafkaConsumer; print('Kafka OK')" 2>/dev/null; then
        echo -e "${GREEN}‚úÖ Kafka connection OK${NC}"
    else
        echo -e "${RED}‚ùå Kafka connection failed${NC}"
    fi
    
    echo -e "\n${YELLOW}4. Testing Spark Session:${NC}"
    if docker logs spark-processor 2>&1 | grep -q "Spark session created"; then
        echo -e "${GREEN}‚úÖ Spark session created${NC}"
    else
        echo -e "${RED}‚ùå Spark session failed${NC}"
    fi
}

# Function to show tutorial
show_tutorial() {
    echo -e "${BLUE}üìö SPARK TUTORIAL - C∆† B·∫¢N${NC}"
    echo "=============================="
    echo ""
    echo -e "${YELLOW}üéØ SPARK L√Ä G√å?${NC}"
    echo "  - Apache Spark: Framework x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn"
    echo "  - In-memory processing: X·ª≠ l√Ω nhanh h∆°n Hadoop"
    echo "  - Real-time streaming: X·ª≠ l√Ω d·ªØ li·ªáu real-time"
    echo "  - Machine Learning: H·ªó tr·ª£ ML algorithms"
    echo ""
    echo -e "${YELLOW}üèóÔ∏è  SPARK ARCHITECTURE:${NC}"
    echo "  - Master: Qu·∫£n l√Ω cluster, schedule jobs"
    echo "  - Worker: Th·ª±c thi tasks, cung c·∫•p resources"
    echo "  - Driver: Ch·∫°y main application"
    echo "  - Executor: Ch·∫°y tasks tr√™n workers"
    echo ""
    echo -e "${YELLOW}üìä SPARK COMPONENTS:${NC}"
    echo "  - Spark Core: Engine c∆° b·∫£n"
    echo "  - Spark SQL: X·ª≠ l√Ω structured data"
    echo "  - Spark Streaming: Real-time processing"
    echo "  - MLlib: Machine Learning library"
    echo "  - GraphX: Graph processing"
    echo ""
    echo -e "${YELLOW}üîÑ DATA PROCESSING FLOW:${NC}"
    echo "  Data Source ‚Üí Spark ‚Üí Processing ‚Üí Output"
    echo "  (Kafka) ‚Üí (Streaming) ‚Üí (Transform) ‚Üí (Database)"
    echo ""
    echo -e "${YELLOW}üí° TRONG PROJECT C·ª¶A B·∫†N:${NC}"
    echo "  - Kafka: Ngu·ªìn d·ªØ li·ªáu real-time"
    echo "  - Spark Streaming: X·ª≠ l√Ω d·ªØ li·ªáu stock"
    echo "  - PostgreSQL: L∆∞u tr·ªØ k·∫øt qu·∫£"
    echo "  - Snowflake: Data warehouse"
}

# Function to show examples
show_examples() {
    echo -e "${BLUE}üí° SPARK EXAMPLES${NC}"
    echo "=================="
    echo ""
    echo -e "${YELLOW}1. Simple Word Count:${NC}"
    echo "  # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´"
    echo "  text_file = spark.read.text('file.txt')"
    echo "  words = text_file.select(explode(split(text_file.value, ' ')).alias('word'))"
    echo "  word_counts = words.groupBy('word').count()"
    echo ""
    echo -e "${YELLOW}2. Kafka Streaming:${NC}"
    echo "  # ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka"
    echo "  kafka_df = spark.readStream.format('kafka')"
    echo "    .option('kafka.bootstrap.servers', 'kafka:9092')"
    echo "    .option('subscribe', 'stock-quotes')"
    echo "    .load()"
    echo ""
    echo -e "${YELLOW}3. Database Write:${NC}"
    echo "  # Ghi d·ªØ li·ªáu v√†o PostgreSQL"
    echo "  df.write.format('jdbc')"
    echo "    .option('url', 'jdbc:postgresql://postgres:5432/stock_db')"
    echo "    .option('dbtable', 'realtime_quotes')"
    echo "    .save()"
}

# Main command handler
case "$1" in
    "status")
        show_status
        ;;
    "ui")
        open_ui
        ;;
    "logs")
        show_logs
        ;;
    "metrics")
        show_metrics
        ;;
    "start")
        start_cluster
        ;;
    "stop")
        stop_cluster
        ;;
    "restart")
        restart_cluster
        ;;
    "test")
        test_cluster
        ;;
    "tutorial")
        show_tutorial
        ;;
    "examples")
        show_examples
        ;;
    "help"|"--help"|"-h"|"")
        show_help
        ;;
    *)
        echo -e "${RED}‚ùå Unknown command: $1${NC}"
        echo "Use '$0 help' to see available commands"
        ;;
esac



